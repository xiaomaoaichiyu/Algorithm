# 字符编码&Python编码

> 之前有看过相关的文章，但是最近遇到了还是不会，所以写成博客，来提醒自己！才疏学浅，理解可能不到位，望大家自己思考！

## 1. 多种编码 and 为什么需要这么多

计算机上面的存储都是01，这对于计算机来说是很好“理解”的，但是对于人类来说就不同了，我们很难从长长的01串中立即获得自己需要的信息，所以**编码**就诞生了。编码是为了让我们能够方便的阅读，把我们熟悉的字符和计算机中的数字（01串）对应起来。

比如我们用数字97来代表字母a，存储在计算机上就是01100001，计算机给人看的时候，不会直接呈现01100001，而是会进行**解码**（编码的逆操作）把存储的01100001变为'a'呈现给我们。

> 一个例子就是，你常常会遇到乱码，这时你会想怎么才能够呈现出我想要的样子呢，让我能够看懂，这就是编码造成的原因，也需要**编码**来帮你！究其原因是因为不同编码所导致。

### 1.1 编码发展历史

这里只介绍ASCII、Unicode、UTF-8。其他的见[万能的百度](<https://baike.baidu.com/item/%E7%BC%96%E7%A0%81/80092?fr=aladdin>)和[更好的Wiki](<https://zh.wikipedia.org/wiki/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81>)!

其他的参考：[阮一峰](<http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html>)，[英文参考](<https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/>)（**推荐阅读**）

> Please do not write another line of code until you finish reading this article.
>
> <right>——见英文参考</right>

#### 1.1.1 ASCII

计算机最早由美国人发明，所以一开始只需要对字母和必要的控制字符进行编码，同时最早的计算机采用8个bit作为1个byte，所以一个字节能够代表256个不同的字符，这对于字母来说足够了，这种编码称为**ASCII**编码。

#### 1.1.2 [Unicode](<https://home.unicode.org/>)

但是问题就是随着计算机的普及，不同国家都需要编码来解决字符问题，而且其他语言不是1个byte就能够搞定的（8位最多映射256个字符，而中文、日语等基本字符远超256）。

因此，又出现了不同的编码格式，GBK、Shift_JIS、Euc-kr，每个国家都搞了一个自己的来用，新的问题就是如何兼容？（兼容性是一个大问题，涉及到国际化使用）。

Unicode应运而生，把所有的语言都编码到一起，这样全部使用Unicode就不会出现因不兼容导致的乱码现象。

Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。[参考](<https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896>)

同时为了兼容ASCII，Unicode里字母和基本的控制字符对应的编码不变，只是在前面加0就可以。

> ASCII：'a' -> 01100001
>
> Unicode：'a' -> 00000000 01100001

#### 1.1.3 UTF-8

然而事情没有那么简单，采用Unicode有什么问题呢？仔细看上面的例子。

如果你是一个**英语母语**的人，采用Unicode存储，你会发现自己白白存储了一大堆的0，是你你干吗？Unicode编码在这种情况下，存储和传输都很浪费。

于是，**UTF-8**又出现了，其主要目的就是**节省**。UTF-8把Unicode字符编码为不同的大小，常用的英文字母编码为1个byte，越是偏僻的字符越大，汉字通常是3个字节。这样的话，如果是大量的英文字母，和ASCII的效率几乎一致。

> 事实上Unicdoe的中文字符字节比utf-8更小，所以传输和存储中文时Unicode应该是更优的，但是为什么现实不这么做呢？我目前的探索和理解是因为行业标准不在我们手里，对于西方国家来说utf-8更加好（都是字母），所以utf-8被认为是更好的传输编码。**这里就体现出行业标准的重要性**！

同样，UTF-8兼容ASCII，过去很多使用ASCII的软件还可以在UTF-8编码下正常使用。

### 1.2 存储，内存，传输

下面总结一下计算机在不同情况下采用的编码：

- 当把文件加载到内存中的时候，这时往往是给用户呈现，所以是**Unicode**编码
- 当存储和传输文件的时候，用户看不到，关心的是兼容和节省空间（流量），所以采用**UTF-8**编码

<br>

********

下面的图来自[廖雪峰官网](<https://www.liaoxuefeng.com/wiki/1016959663602400/1017075323632896>)

![1587345939450](C:\Users\legend\AppData\Roaming\Typora\typora-user-images\1587345939450.png)

![1587345954290](C:\Users\legend\AppData\Roaming\Typora\typora-user-images\1587345954290.png)

****

<br>

所以，硬盘上的文件往往是UTF-8编码存储，网络传输往往是UTF-8传输（如果确定完全是英文可以采用ASCII，当然不建议这么做，你永远不知道对方会怎么做，所以你最好处理好一切！）

而你在内存中打开的文件往往是Unicode的（当然我们也可以指定打开文件时采用的编码方式，使得内存中的文件是**UTF-8**编码等）



## 2. python字符编码

### 2.1 简单扩展 [r'', b'', u'', f''](<https://blog.csdn.net/u010709324/article/details/100163873>)

python的字符串前面会有r、b、u、f来指定字符串的特性

- r'   ' : 非转义的原始字符串。这种情况下字符串中转义字符都是普通的字符，不再具有转义性

  > print(r'this is a test!\n')
  >
  > 输出：'this is a test!\n'

- b'   ' : bytes。python3默认的字符串str是Unicode编码；python2中默认字符串str是bytes。所以b'string'代表该字符串不再是Unicode编码，而是字节（ASCII或者utf-8）

  > ![1587347146715](C:\Users\legend\AppData\Roaming\Typora\typora-user-images\1587347146715.png)
  >
  > ![1587346980448](C:\Users\legend\AppData\Roaming\Typora\typora-user-images\1587346980448.png)

这里留个坑（bytes的行为比较怪异，ASCII和UTF-8都支持，但是Unicode会报错，之后补充）

- u'   ' : 表示Unicode字符串

  > ![1587348094950](C:\Users\legend\AppData\Roaming\Typora\typora-user-images\1587348094950.png)

- f'   ' : 格式化字符串，类似format()

>  chr() 和 ord()函数的应用！

### 2.2 

**Python3的字符串类型是str，采用的是Unicode编码**。而python2字符串默认是bytes类型。

所以python3中的'ABC'和b'ABC'是不一样的，类型是不同的 （b' '是什么请看上面的介绍），前者是str，后者是bytes。

下面通过几个例子来解释这些：

```python
>>>print('中国')
中国
#这里字符串'中国'是Unicode编码，内存中的值应该是'\u4e2d\u56fd'

>>>print('中国nb!'.encode('utf-8'))
b'\xe4\xb8\xad\xe5\x9b\xbdnb!'
#我们看到转换为utf-8后输出的是一个bytes类型的字符串，其中b'\xe4\xb8\xad'对应'中'，b'\xe5\x9b\xbd'对应'国';
#'nb!'因为是ASCII中的字符所以可以正常输出

#我们会发现Unicode的'中'和utf-8的'中'的字节是不一样的
>>>print('\u4e2d\u56fd') #Unicdoe
中国
>>>print(b'\xe4\xb8\xad\xe5\x9b\xbd'.decode('utf-8')) #utf-8
中国
```

```python
>>>'中国'.encode('utf-8')
b'\xe4\xb8\xad\xe5\x9b\xbd'
>>>'中国'.encode('ascii')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)

#这个例子建议细品，原因错误已经给出了！将'中国'编码为ascii遇到超过128的字节将处理不了！
```

python中的字符串是一堆符号——'xxxxxxx'，我们对于字符串的理解取决于编码方式，从文件加载到内存的字符串（默认是Unicode编码），我们看到的就是Unicode编码下的字符串，如果使用encode进行转换，就可以看到其他的编码下的字符串，这里编码用错了就会看到截然不同的字符串：

unicode-escape编码是将**unicode内存编码值**直接存储

```python
>>> decode_word = u'我是程序员'.encode('unicode-escape')
>>> decode_word
b'\\u6211\\u662f\\u7a0b\\u5e8f\\u5458'
>>>len(b'\\u6211\\u662f\\u7a0b\\u5e8f\\u5458')
30
#\\中的第一个\是转义字符，注意现在看到的都是字符，并不是计算机存储的数字(十六进制或者二进制)
```



关于python的编码，感觉还不是很明白，有很多坑没填上，择日再战！

<br>

参考[python地址内数据查看](<https://www.cnblogs.com/shine-lee/p/12286856.html>)